<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="William Schultz" />
  <meta name="author" content="William Schultz" />
  <title>Learning representations by back-propagating errors</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <link rel="stylesheet" href="../../style.css" />
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Learning representations by back-propagating errors</h1>
<p class="author">William Schultz</p>
<p class="author">William Schultz</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#bibliography">References</a></li>
</ul>
</nav>
<p>This paper <span class="citation" data-cites="rumelhart1986learning">(<a href="#ref-rumelhart1986learning" role="doc-biblioref">Rumelhart, Hinton, and Williams 1986</a>)</span> describes a new learning procedure, <em>back-propagation</em>, for networks of neurone-like units (i.e. neural networks).</p>
<p>Layered neural networks are defined as follows. They consist of a layer of input units at the bottom, any number of intermediate layers, and a layer of output units at the top. Connections within a layer or from higher to lower layers are forbidden (i.e. they are “feed-forward” networks). The total input <span class="math inline">\(x_j\)</span> to unit <span class="math inline">\(j\)</span> is a linear function of the outputs <span class="math inline">\(y_i\)</span> of the units connected to <span class="math inline">\(j\)</span> from the previous layer, and of the weights <span class="math inline">\(w_{ji}\)</span> on the connections coming into <span class="math inline">\(j\)</span>: <span class="math display">\[\begin{aligned}
    x_j = \sum_{i} y_i w_{ij}\end{aligned}\]</span> A unit’s final output is a real value, <span class="math inline">\(y_j\)</span>, which is a non-linear function of the total input computed from the above equation <span class="math display">\[\begin{aligned}
    y_j = \dfrac{1}{1+e^{-x_j}}\end{aligned}\]</span></p>
<p>The overall goal is to find a set of weights so as to minimize error with respect to a set of given, finite set of samples (input-output cases). The total error <span class="math inline">\(E\)</span>, is defined as <span class="math display">\[\begin{aligned}
    E = \frac{1}{2} \sum_{x}\sum_{j} (y_{j,c} - d_{j,c})^2
    \label{eq:error}\end{aligned}\]</span> where <span class="math inline">\(j\)</span> ranges over the set of output values and <span class="math inline">\(c\)</span> ranges over each input-output sample, <span class="math inline">\(y\)</span> is the state of an output unit, and <span class="math inline">\(d\)</span> is the desired state (as given by the sample).</p>
<p>Then, to minimize <span class="math inline">\(E\)</span> by gradient descent, we must compute the partial derivative of <span class="math inline">\(E\)</span> with respect to each weight in the network. This is simply the sum of the partial derivatives for each of the input-output cases.</p>
<section id="computing-the-gradient" class="level4 unnumbered">
<h4 class="unnumbered">Computing the gradient</h4>
<p>Forward pass: Add note.</p>
<p>The backward pass starts by computing <span class="math inline">\(\partial E / \partial y\)</span> for each of the output units. Differentiating Equation <a href="#eq:error" data-reference-type="ref" data-reference="eq:error">[eq:error]</a> gives us <span class="math display">\[\begin{aligned}
    \partial E / \partial = y_j - d_j\end{aligned}\]</span> and then we can apply the chain rule to compute <span class="math inline">\(\partial E / \partial x_j\)</span> <span class="math display">\[\begin{aligned}
    \partial E / \partial x_j = \partial E / \partial y_j \cdot dy_j / dx_j\end{aligned}\]</span></p>
<p>TODO.</p>
</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-rumelhart1986learning" class="csl-entry" role="doc-biblioentry">
Rumelhart, David E, Geoffrey E Hinton, and Ronald J Williams. 1986. <span>“Learning Representations by Back-Propagating Errors.”</span> <em>Nature</em> 323 (6088): 533–36.
</div>
</div>
</section>
</body>
</html>
