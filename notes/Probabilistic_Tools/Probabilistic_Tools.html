<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="William Schultz" />
  <meta name="author" content="William Schultz" />
  <title>Probabilistic Tools</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <link rel="stylesheet" href="../../style.css" />
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Probabilistic Tools</h1>
<p class="author">William Schultz</p>
<p class="author">William Schultz</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#linearity-of-expectation">Linearity of Expectation</a></li>
<li><a href="#expectation-and-variance">Expectation and Variance</a></li>
<li><a href="#markovs-inequality">Markov’s Inequality</a></li>
<li><a href="#chebyshevs-inequality">Chebyshev’s Inequality</a></li>
<li><a href="#chernoff-bound">Chernoff Bound</a></li>
<li><a href="#union-bound">Union Bound</a></li>
<li><a href="#bibliography">References</a></li>
</ul>
</nav>
<p>A <em>random variable</em> is a variable whose possible values are numerical outcomes of a random phenomenon. Each possible outcome is assigned some probability, with the condition that the sum of probabilities overall possible outcomes must sum to 1.<br />
See <span class="citation" data-cites="Doerr_2019">(<a href="#ref-Doerr_2019" role="doc-biblioref">Doerr 2019</a>)</span> for a reference on some probabilistic tools.</p>
<section id="linearity-of-expectation" class="level2 unnumbered">
<h2 class="unnumbered">Linearity of Expectation</h2>
<p>For any two random variables <span class="math inline">\(X,Y\)</span>, it holds that <span class="math display">\[\begin{aligned}
    \mathbb{E}[X + Y] =  \mathbb{E}[X] + \mathbb{E}[Y]\end{aligned}\]</span></p>
</section>
<section id="expectation-and-variance" class="level2 unnumbered">
<h2 class="unnumbered">Expectation and Variance</h2>
<p>For a discrete random variable <span class="math inline">\(X\)</span> taking values in some set <span class="math inline">\(\Omega \subseteq \mathbb{R}\)</span>, its <em>expectation</em> is defined as <span class="math display">\[\begin{aligned}
    \mathbb{E}\left[X\right] = \sum_{\omega \in \Omega} \omega \cdot Pr\left[X=\omega\right]\end{aligned}\]</span> and its <em>variance</em> is defined as <span class="math display">\[\begin{aligned}
    \text{Var}(X) = \mathbb{E}\left[(X - \mathbb{E}\left[X\right])^2\right]\end{aligned}\]</span> That is, expectation is essentially a weighted sum of the values that the random variable can take on, where each value is weighted by the probability of that event occurring, and variance is essentially a measure of the average deviation of the variable from its expectation/mean.</p>
</section>
<section id="markovs-inequality" class="level2 unnumbered">
<h2 class="unnumbered">Markov’s Inequality</h2>
<p><em>Markov’s inequality</em> is an elementary large deviation bound valid for <em>all</em> non-negative random variables. Let <span class="math inline">\(X\)</span> be a non-negative random variable with <span class="math inline">\(\mathbb{E}\left[X\right] &gt; 0\)</span>. Then, for all <span class="math inline">\(\lambda &gt; 0\)</span> <span class="math display">\[\begin{aligned}
    Pr(X \geq \lambda \mathbb{E}\left[X\right]) \leq \dfrac{1}{\lambda}\end{aligned}\]</span> That is, this establishes a bound, for some given parameter <span class="math inline">\(\lambda\)</span>, on how likely a random variable is to be far away from its mean. For example, if <span class="math inline">\(\lambda = 10\)</span>, then this means that the probability that <span class="math inline">\(X\)</span> is greater than 10 times its mean is <span class="math inline">\(\leq \frac{1}{10}\)</span>. Note that this bound doesn’t take into account anything about the actual distribution (only about its mean), so it may serve as only a rough estimate.</p>
</section>
<section id="chebyshevs-inequality" class="level2 unnumbered">
<h2 class="unnumbered">Chebyshev’s Inequality</h2>
<p>Let <span class="math inline">\(X\)</span> be a random variable with <span class="math inline">\(\text{Var}(X) &gt; 0\)</span>, and where <span class="math inline">\(\sigma = \sqrt{\text{Var}(X)}\)</span> is its standard deviation. Then, for all <span class="math inline">\(\lambda &gt; 0\)</span> <span class="math display">\[\begin{aligned}
    Pr \left[ \left| X - \mathbb{E}\left[X\right] \right| \geq \lambda \sigma \right] \leq \dfrac{1}{\lambda^2}\end{aligned}\]</span> This bound tells us something about how far a random variable is from its expectation, in terms of the variance of the RV. That is, it puts a bound on the probability of how many (<span class="math inline">\(\lambda\)</span>) standard deviations (<span class="math inline">\(\sigma\)</span>) away from its mean <span class="math inline">\(X\)</span> may be.</p>
</section>
<section id="chernoff-bound" class="level2 unnumbered">
<h2 class="unnumbered">Chernoff Bound</h2>
<p>Let <span class="math inline">\(X_1,\dots,X_n\)</span> be independent random variables in <span class="math inline">\([0,1]\)</span>. Let <span class="math inline">\(X = \sum_{i=1}^n X_i\)</span> and let <span class="math inline">\(\mu = \mathbb{E}[X]\)</span>. Then</p>
<ul>
<li><p><span class="math inline">\(\quad 
    Pr[X \geq (1+\delta)\mu] \leq \exp\left({-\tfrac{\delta^2}{2+\delta}\mu}\right), \quad  \forall \delta &gt; 0\)</span></p></li>
<li><p><span class="math inline">\(\quad
    Pr[X \geq (1-\delta)\mu] \leq \exp\left({-\tfrac{\delta^2}{2}\mu}\right), \quad  \forall \delta \in [0,1]\)</span></p></li>
</ul>
<p>The two tail bounds above can be combined into the following (slightly weaker) two-sided tail bound:</p>
<ul>
<li><p><span class="math inline">\(\quad 
    Pr[|X-\mu| \geq \delta \mu] \leq 2 \exp\left({-\tfrac{\delta^2 \mu}{3}}\right), \quad  \forall \delta \in [0,1]\)</span></p></li>
</ul>
<p>We can also derive the <em>additive</em> form of the bound by setting <span class="math inline">\(t=\delta \mu\)</span>:</p>
<ul>
<li><p><span class="math inline">\(\quad 
    Pr[|X-\mu| \geq t] \leq 2 \exp \left( {-\tfrac{t^2}{3 \mu}} \right), \quad  \forall t \in [0,\mu]\)</span></p></li>
</ul>
</section>
<section id="union-bound" class="level2 unnumbered">
<h2 class="unnumbered">Union Bound</h2>
<p>Let <span class="math inline">\(E_1,\dots,E_n\)</span> be arbitrary events in some probability space. Then, <span class="math display">\[\begin{aligned}
    Pr \left[ \bigcup_{i=1}^n E_i \right] \leq \sum_{i=1}^n Pr[E_i]\end{aligned}\]</span> That is, for a set of events <span class="math inline">\(E_1,\dots,E_n\)</span>, the probability of at least one event occurring is less than or equal to the sum of the probabilities of the individual events. We can think of the union bound with a simple set-based analogy i.e. if events are viewe as subsets of a potential sample space, then the union of these sets can be no larger than the sum of the individual sets (i.e. since the sets may have some non-empty intersections).</p>
</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Doerr_2019" class="csl-entry" role="doc-biblioentry">
Doerr, Benjamin. 2019. <span>“Probabilistic Tools for the Analysis of Randomized Optimization Heuristics.”</span> In <em>Natural Computing Series</em>, 1–87. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-030-29414-4_1">https://doi.org/10.1007/978-3-030-29414-4_1</a>.
</div>
</div>
</section>
</body>
</html>
