\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{fancyvrb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes,snakes}
\usepackage[english]{babel}

\geometry{legalpaper, margin=1.5in}

\author{William Schultz}
\begin{document}
\title{Probabilistic Tools}
\author{William Schultz}
\maketitle

\newcommand{\expect}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\var}[1]{\text{Var}(#1)}

A \textit{random variable} is a variable whose possible values are numerical outcomes of a random phenomenon. Each possible outcome is assigned some probability, with the condition that the sum of probabilities overall possible outcomes must sum to 1.\\See \cite{Doerr_2019} for a reference on some probabilistic tools.

\subsection*{Linearity of Expectation}
For any two random variables $X,Y$, it holds that
\begin{align*}
    \mathbb{E}[X + Y] =  \mathbb{E}[X] + \mathbb{E}[Y]
\end{align*}

\subsection*{Expectation and Variance}

For a discrete random variable $X$ taking values in some set $\Omega \subseteq \mathbb{R}$, its \textit{expectation} is defined as
\begin{align*}
    \expect{X} = \sum_{\omega \in \Omega} \omega \cdot Pr\left[X=\omega\right]
\end{align*}
and its \textit{variance} is defined as 
\begin{align*}
    \var{X} = \expect{(X - \expect{X})^2}
\end{align*}
That is, expectation is essentially a weighted sum of the values that the random variable can take on, where each value is weighted by the probability of that event occurring, and variance is essentially a measure of the average deviation of the variable from its expectation/mean.

\subsection*{Markov's Inequality}

\textit{Markov's inequality} is an elementary large deviation bound valid for \textit{all} non-negative random variables. Let $X$ be a non-negative random variable with $\expect{X} > 0$. Then, for all $\lambda > 0$
\begin{align*}
    Pr(X \geq \lambda \expect{X}) \leq \dfrac{1}{\lambda}
\end{align*}
That is, this establishes a bound, for some given parameter $\lambda$, on how likely a random variable is to be far away from its mean. For example, if $\lambda = 10$, then this means that the probability that $X$ is greater than 10 times its mean is $\leq \frac{1}{10}$. Note that this bound doesn't take into account anything about the actual distribution (only about its mean), so it may serve as only a rough estimate.

\subsection*{Chebyshev's Inequality}

Let $X$ be a random variable with $\var{X} > 0$, and where $\sigma = \sqrt{\var{X}}$ is its standard deviation. Then, for all $\lambda > 0$
\begin{align*}
    Pr \left[ \left| X - \expect{X} \right| \geq \lambda \sigma \right] \leq \dfrac{1}{\lambda^2}
\end{align*}
This bound tells us something about how far a random variable is from its expectation, in terms of the variance of the RV. That is, it puts a bound on the probability of how many ($\lambda$) standard deviations ($\sigma$) away from its mean $X$ may be.

% Note that Chebyshev's inequality can be derived from Markov's inequality. Start with
% \begin{align*}
%     Pr(X \geq \lambda \expect{X}) \leq \dfrac{1}{\lambda}
% \end{align*}

\subsection*{Chernoff Bound}

Let $X_1,\dots,X_n$ be independent random variables in $[0,1]$. Let $X = \sum_{i=1}^n X_i$ and let $\mu = \mathbb{E}[X]$. Then
\begin{itemize}
    \item \textbf{Upper Tail}
    $Pr[X \geq (1+\delta)\mu] \leq \exp\left({-\tfrac{\delta^2}{2+\delta}\mu}\right), \quad  \forall \delta > 0$
    \item \textbf{Lower Tail}:
    $Pr[X \geq (1-\delta)\mu] \leq \exp\left({-\tfrac{\delta^2}{2}\mu}\right), \quad  \forall \delta \in [0,1]$
\end{itemize}
The two tail bounds above can be combined into the following (slightly weaker) two-sided tail bound:
\begin{itemize}
    \item \textbf{Two-sided bound}: $Pr[|X-\mu| \geq \delta \mu] \leq 2 \exp\left({-\tfrac{\delta^2 \mu}{3}}\right), \quad  \forall \delta \in [0,1]$
\end{itemize}
We can also derive the \textit{additive} form of the bound by setting $t=\delta \mu$:
\begin{itemize}
    \item \textbf{Additive Chernoff bound}: 
    $Pr[|X-\mu| \geq t] \leq 2 \exp \left( {-\tfrac{t^2}{3 \mu}} \right), \quad  \forall t \in [0,\mu]$
\end{itemize}

\subsection*{Union Bound}

Let $E_1,\dots,E_n$ be arbitrary events in some probability space. Then,
\begin{align*}
    Pr \left[ \bigcup_{i=1}^n E_i \right] \leq \sum_{i=1}^n Pr[E_i]
\end{align*}
That is, for a set of events $E_1,\dots,E_n$, the probability of at least one event occurring is less than or equal to the sum of the probabilities of the individual events. We can think of the union bound with a simple set-based analogy i.e. if events are viewe as subsets of a potential sample space, then the union of these sets can be no larger than the sum of the individual sets (i.e. since the sets may have some non-empty intersections).


\bibliographystyle{plain}
\bibliography{../../references.bib}

\end{document}