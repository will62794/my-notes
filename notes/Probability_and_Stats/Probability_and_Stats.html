<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="William Schultz" />
  <meta name="author" content="William Schultz" />
  <title>Probability and Stats</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="../../style.css" />
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Probability and Stats</h1>
<p class="author">William Schultz</p>
<p class="author">William Schultz</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#random-variables"><span class="toc-section-number">1</span> Random Variables</a>
<ul>
<li><a href="#expected-value"><span class="toc-section-number">1.1</span> Expected Value</a></li>
</ul></li>
<li><a href="#some-examples">Some Examples</a></li>
<li><a href="#conditional-probability-and-bayes-formula"><span class="toc-section-number">2</span> Conditional Probability and Bayes’ Formula</a>
<ul>
<li><a href="#bayes-rule"><span class="toc-section-number">2.1</span> Bayes’ Rule</a></li>
</ul></li>
<li><a href="#markov-chains"><span class="toc-section-number">3</span> Markov Chains</a></li>
</ul>
</nav>
<section id="random-variables" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Random Variables</h1>
<p>We can model probabilistic events based on <em>outcomes</em>, which are the outcome of an experiment or a trial (e.g., getting a “3” on the roll of a die), and a <em>sample space</em>, which is a set of possible outcomes, and <em>events</em>, which are a subset of the sample space.</p>
<p>A <strong>random variable</strong> is then, formally, simply a mapping from outcomes in the sample space to the set of real numbers. For example consider a 6-sided die with sides <span class="math inline">\(\{1,2,3,4,5,6\}\)</span>. Some possible outcomes are <span class="math inline">\(1\)</span>, <span class="math inline">\(3\)</span>, or <span class="math inline">\(5\)</span>, and the sample space is <span class="math inline">\(\{1,2,3,4,5,6\}\)</span>. The probability of each outcome for a fair die, deteremined by some random variable, is <span class="math inline">\(1/6\)</span>. We can define an event <span class="math inline">\(A\)</span> representing the case that the roll is odd i.e., <span class="math inline">\(A=\{1,3,5\}\)</span>.</p>
<section id="expected-value" class="level2" data-number="1.1">
<h2 data-number="1.1"><span class="header-section-number">1.1</span> Expected Value</h2>
<p>The <strong>expected value</strong> of a random variable <span class="math inline">\(X\)</span> can be thought of as the “average” or “mean” value attained by that random variable. Formally, expected value is defined as <span class="math display">\[\begin{aligned}
    E[X] = \sum_{x} x * P(X=x)\end{aligned}\]</span> for every outcome <span class="math inline">\(x\)</span> in the sample space of <span class="math inline">\(X\)</span>. For example, for a fair, 6-sided die, the expected value is <span class="math display">\[\begin{aligned}
  \frac{1}{6}*1 + \frac{1}{6}*2 + \frac{1}{6}*3 + \frac{1}{6}*4 + \frac{1}{6}*5 + \frac{1}{6}*6 = 3.5\end{aligned}\]</span> Or if we have, say, a 4-sided die where each roll has some “payoff”, e.g.</p>
<ul>
<li><p>1 <span class="math inline">\(\mapsto\)</span> 1</p></li>
<li><p>2 <span class="math inline">\(\mapsto\)</span> 2</p></li>
<li><p>3 <span class="math inline">\(\mapsto\)</span> 3</p></li>
<li><p>4 <span class="math inline">\(\mapsto\)</span> 10</p></li>
</ul>
<p>our expected payoff from rolling this die is <span class="math display">\[\begin{aligned}
    \frac{1}{4}*1 + \frac{1}{4}*2 + \frac{1}{4}*3 + \frac{1}{4}*10 = 4\end{aligned}\]</span></p>
</section>
</section>
<section id="some-examples" class="level1 unnumbered">
<h1 class="unnumbered">Some Examples</h1>
<section id="coin-toss-game" class="level4" data-number="1.1.0.1">
<h4 data-number="1.1.0.1"><span class="header-section-number">1.1.0.1</span> Coin Toss Game</h4>
<p>Assume there are two gamblers playing a coin toss game. Gambler <span class="math inline">\(A\)</span> has <span class="math inline">\((n+1)\)</span> fair coins, and <span class="math inline">\(B\)</span> has <span class="math inline">\(n\)</span> fair coins. What is the probability that <span class="math inline">\(A\)</span> will have more heads than <span class="math inline">\(B\)</span> if both flip all of their coins?</p>
<p>We cna look at this problem symmetrically if we imagine a slightly simplified scenario where <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> both have <span class="math inline">\(n\)</span> coins. Then, we know that for the following 3 scenarios</p>
<ul>
<li><p><span class="math inline">\(A\)</span> flips more heads than <span class="math inline">\(B\)</span>.</p></li>
<li><p><span class="math inline">\(A\)</span> flips the same number of heads as <span class="math inline">\(B\)</span>.</p></li>
<li><p><span class="math inline">\(A\)</span> flips fewer heads than <span class="math inline">\(B\)</span>.</p></li>
</ul>
<p>the first and third are symmetric, and so must have equal probabilities. Then, we really only need to consider the second case, which is the case where <span class="math inline">\(A\)</span>’s <span class="math inline">\(n+1\)</span>-th flip would actually make a difference.</p>
</section>
<section id="card-game" class="level4 unnumbered">
<h4 class="unnumbered">Card Game</h4>
<p>A casino offers a card game from a deck of 52 cards with 4 cards each for <span class="math inline">\(2,3,4,5,6,7,8,9,10,J,Q,K,A\)</span>. You pick up a card from the deck and the dealer picks another one with replacement. What is the probability that you picked a larger card than the dealer’s?</p>
<p>Well, one way to analyze this is by considering all possible choices of two cards <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span>. The probability of pciking any particular suit for <span class="math inline">\(C_1\)</span> is <span class="math inline">\(4/52 = 1/13\)</span>. We can choose to then simply analyze the probability of getting a smaller card for each possible suit case. Basically, for each suit, the probability of picking a lesser card for <span class="math inline">\(C_2\)</span> is <span class="math inline">\(4/51 * k\)</span>, where <span class="math inline">\(k\)</span> is 1 less than the rank of the suit. So, in general, we can compute the overall probability as <span class="math display">\[\begin{aligned}
    \left(\frac{1}{13}*\frac{4}{51}*0\right) + \left(\frac{1}{13}*\frac{4}{51}*1\right) + \left(\frac{1}{13}*\frac{4}{51}*2\right) + \dots + \left(\frac{1}{13}*\frac{4}{51}*12\right) = 0.4705\end{aligned}\]</span></p>
</section>
</section>
<section id="conditional-probability-and-bayes-formula" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Conditional Probability and Bayes’ Formula</h1>
<p>In standard probability notions, we may have some event <span class="math inline">\(A\)</span> and want to know the probability of <span class="math inline">\(A\)</span> occurring. If, however, we have two separate events, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, we can also consider the probability that one event occurs <em>given</em> that the other occurs. This is known as conditional probability i.e., for events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, the conditional probability of <span class="math inline">\(A\)</span> <em>given</em> <span class="math inline">\(B\)</span> is defined as <span class="math display">\[\begin{aligned}
    P(A \mid B) = \dfrac{P(A \cap B)}{P(B)}\end{aligned}\]</span> You can think of this formula visually:</p>
<div class="center">
<p><img src="diagrams/prob_diagrams/prob_diagrams.001.png" alt="image" /></p>
</div>
<p>That is, if we consider <span class="math inline">\(P(A | B)\)</span>, we can imagine this is like saying our sample space is now shrunk just to <span class="math inline">\(B\)</span>, and so the probability of <span class="math inline">\(A\)</span> in this reduced space is dependent on the size of the intersection between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> as a proportion of <span class="math inline">\(B\)</span>, as represented in the formula.</p>
<section id="example" class="level4 unnumbered">
<h4 class="unnumbered">Example</h4>
<p>For example, if I roll a fair 6-sided die, then the probability of rolling a 4 is <span class="math inline">\(1/6\)</span>. But, if I tell you upfront that the die roll seen is <span class="math inline">\(&gt;= 4\)</span>, then the probability that the roll is a 4 is actually now <span class="math inline">\(1/3\)</span>, since there are now only 3 possible outcomes in the reduced/conditioned sample sample (<span class="math inline">\(\{4,5,6\}\)</span>) and our outcome 4 is one of them.</p>
</section>
<section id="bayes-rule" class="level2" data-number="2.1">
<h2 data-number="2.1"><span class="header-section-number">2.1</span> Bayes’ Rule</h2>
<p><strong>Bayes’ rule</strong> provides an alternate expression for conditional probability. For two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, it states: <span class="math display">\[\begin{aligned}
    P(A \mid B) = \dfrac{P(B|A) * P(A)}{P(B)}\end{aligned}\]</span> You can think of this as deriving from the basic conditional probability statement above since <span class="math inline">\(P(B \mid A)\)</span> can be viewed as the probability of the intersection between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> as a proportion to <span class="math inline">\(A\)</span> as our sample space, and then we scale this up by factor of <span class="math inline">\(P(A)\)</span> to explicitly get <span class="math inline">\(P(A \cap B)\)</span> in the numerator.</p>
</section>
</section>
<section id="markov-chains" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Markov Chains</h1>
<p>A <strong>Markov chain</strong> is a sequence of random variables <span class="math inline">\(X_0,X_1,\dots,X_n,\dots\)</span> with the Markov property that, given the present state, the future states and past states are independent. In other words, once the current state is known, past history has no bearing on the future. A Markov chain with <span class="math inline">\(M\)</span> states can be completely described by an <span class="math inline">\(M \times M\)</span> transition matrix <span class="math inline">\(P\)</span> and the initial probabilities <span class="math inline">\(P(X_0)\)</span>, where <span class="math inline">\(p_{ij}\)</span> is the probability of transitioning from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span>. The probability of a particular path of states then, is the product of the probabilities of taking each corresponding transition in that path.</p>
<p>A <strong>transition graph</strong> is more commonly used to graphically represent the transition matrix i.e.</p>
<div class="center">
<p><img src="diagrams/markov_chain_graph.png" alt="image" /></p>
</div>
<p>Note that a state <span class="math inline">\(i\)</span> is called <strong>absorbing</strong> if it is impossible to leave this state.</p>
<section id="example-gamblers-ruin" class="level4 unnumbered">
<h4 class="unnumbered">Example: Gambler’s Ruin</h4>
<p>As an example, consider the gambler’s ruin problem. Say player <span class="math inline">\(M\)</span> has $1 and play N has $2. Each game gives the winner of that game $1 from the other. As a better player, <span class="math inline">\(M\)</span> wins 2/3 of the games, and they play until one of them is bankrupt. What is the probability that <span class="math inline">\(M\)</span> wins?</p>
<p>We can model this as a Markov chain where states are tuples of the account values of <span class="math inline">\(M\)</span> and <span class="math inline">\(N\)</span>, with initial state of <span class="math inline">\((1,2)\)</span>. At each state, there are then two possible transitions: one where <span class="math inline">\(M\)</span> wins and one where <span class="math inline">\(N\)</span> wins, the former having probability <span class="math inline">\(2/3\)</span> and the latter having probability <span class="math inline">\(1/3\)</span>. The <span class="math inline">\(M\)</span> winning transition goes from a state <span class="math inline">\((m,n)\)</span> to <span class="math inline">\((m+1,n-1)\)</span>, and similarly for the <span class="math inline">\(N\)</span> winning transition. A winning (losing) state is one where one player’s account balance is <span class="math inline">\(\leq 0\)</span>. So, we want to determine the probability of reaching a state where <span class="math inline">\(M\)</span> wins, starting from the initial state.</p>
<p>The Markov chain (i.e. probabilistic transition system) for this problem can be drawn as follows (where bankrupt values are shown in red):</p>
<div class="center">
<p><img src="diagrams/prob_diagrams/prob_diagrams.002.png" alt="image" /></p>
</div>
</section>
</section>
</body>
</html>
