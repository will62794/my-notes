<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="William Schultz" />
  <meta name="author" content="William Schultz" />
  <title>Probability and Stats</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../../style.css" />
  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Probability and Stats</h1>
<p class="author">William Schultz</p>
<p class="author">William Schultz</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#random-variables" id="toc-random-variables"><span
class="toc-section-number">1</span> Random Variables</a>
<ul>
<li><a href="#expected-value" id="toc-expected-value"><span
class="toc-section-number">1.1</span> Expected Value</a></li>
<li><a href="#variance" id="toc-variance"><span
class="toc-section-number">1.2</span> Variance</a></li>
<li><a href="#some-examples" id="toc-some-examples">Some
Examples</a></li>
</ul></li>
<li><a href="#conditional-probability-and-bayes-formula"
id="toc-conditional-probability-and-bayes-formula"><span
class="toc-section-number">2</span> Conditional Probability and Bayes’
Formula</a>
<ul>
<li><a href="#bayes-rule" id="toc-bayes-rule"><span
class="toc-section-number">2.1</span> Bayes’ Rule</a></li>
<li><a href="#examples" id="toc-examples">Examples</a></li>
</ul></li>
<li><a href="#markov-chains" id="toc-markov-chains"><span
class="toc-section-number">3</span> Markov Chains</a></li>
<li><a href="#basic-calculus" id="toc-basic-calculus"><span
class="toc-section-number">4</span> Basic Calculus</a>
<ul>
<li><a href="#common-derivative-rules"
id="toc-common-derivative-rules">Common Derivative Rules</a></li>
<li><a href="#derivatives-of-common-functions"
id="toc-derivatives-of-common-functions">Derivatives of Common
Functions</a></li>
</ul></li>
</ul>
</nav>
<section id="random-variables" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Random
Variables</h1>
<p>We can model probabilistic events based on <em>outcomes</em>, which
are the outcome of an experiment or a trial (e.g., getting a “3” on the
roll of a die), and a <em>sample space</em>, which is a set of possible
outcomes, and <em>events</em>, which are a subset of the sample
space.</p>
<p>A <strong>random variable</strong> is then, formally, simply a
mapping from outcomes in the sample space to the set of real numbers.
For example consider a 6-sided die with sides <span
class="math inline">\(\{1,2,3,4,5,6\}\)</span>. Some possible outcomes
are <span class="math inline">\(1\)</span>, <span
class="math inline">\(3\)</span>, or <span
class="math inline">\(5\)</span>, and the sample space is <span
class="math inline">\(\{1,2,3,4,5,6\}\)</span>. The probability of each
outcome for a fair die, deteremined by some random variable, is <span
class="math inline">\(1/6\)</span>. We can define an event <span
class="math inline">\(A\)</span> representing the case that the roll is
odd i.e., <span class="math inline">\(A=\{1,3,5\}\)</span>.</p>
<section id="expected-value" class="level2" data-number="1.1">
<h2 data-number="1.1"><span class="header-section-number">1.1</span>
Expected Value</h2>
<p>The <strong>expected value</strong> of a random variable <span
class="math inline">\(X\)</span> can be thought of as the “average” or
“mean” value attained by that random variable. Formally, expected value
is defined as <span class="math display">\[\begin{aligned}
    E[X] = \sum_{x} x * P(X=x)
\end{aligned}\]</span> for every outcome <span
class="math inline">\(x\)</span> in the sample space of <span
class="math inline">\(X\)</span>. For example, for a fair, 6-sided die,
the expected value is <span class="math display">\[\begin{aligned}
  \frac{1}{6}*1 + \frac{1}{6}*2 + \frac{1}{6}*3 + \frac{1}{6}*4 +
\frac{1}{6}*5 + \frac{1}{6}*6 = 3.5
\end{aligned}\]</span> Or if we have, say, a 4-sided die where each roll
has some “payoff”, e.g.</p>
<ul>
<li><p>1 <span class="math inline">\(\mapsto\)</span> 1</p></li>
<li><p>2 <span class="math inline">\(\mapsto\)</span> 2</p></li>
<li><p>3 <span class="math inline">\(\mapsto\)</span> 3</p></li>
<li><p>4 <span class="math inline">\(\mapsto\)</span> 10</p></li>
</ul>
<p>our expected payoff from rolling this die is <span
class="math display">\[\begin{aligned}
    \frac{1}{4}*1 + \frac{1}{4}*2 + \frac{1}{4}*3 + \frac{1}{4}*10 = 4
\end{aligned}\]</span></p>
<section id="expected-flips-until-k-heads" class="level4 unnumbered">
<h4 class="unnumbered">Expected flips until <span
class="math inline">\(k\)</span> heads</h4>
<p>As another example, consider computing the expected number of flips
of a fair coin until two heads appear in a row. We can think about this
as a model where we sample by continuing to flip a coin until we get two
heads, and then consider the length of the flip sequence we needed to
get those two heads. Then, we want to consider the average number of
flips it takes over all possible such flips sequences to get two heads
in a row.</p>
<p>One way to compute this probability is by considering it as a Markov
chain and describe the expected path length recursively. That is, there
are 3 cases:</p>
<ul>
<li><p>First two flips are heads: <span
class="math inline">\(p=\frac{1}{4}\)</span> and we are done.</p></li>
<li><p>First flip is heads, second is tails: <span
class="math inline">\(p=\frac{1}{4}\)</span> and we are back to the
start.</p></li>
<li><p>First flip is tails: <span
class="math inline">\(p=\frac{1}{2}\)</span> and we are back to the
start.</p></li>
</ul>
<p>So, we can compute the overall expected path length <span
class="math inline">\(x\)</span> by solving the following: <span
class="math display">\[\begin{aligned}
    x &amp;= \frac{1}{4}\cdot 2 + \frac{1}{4}\cdot (x+2) +
\frac{1}{2}\cdot (x+1) \\
    &amp;= \frac{3}{4}x+\frac{6}{4} \\
    \frac{1}{4}x&amp;= \frac{6}{4} \\
    x&amp;= 6
\end{aligned}\]</span></p>
</section>
</section>
<section id="variance" class="level2" data-number="1.2">
<h2 data-number="1.2"><span class="header-section-number">1.2</span>
Variance</h2>
<p>The <strong>variance</strong> of a random variable <span
class="math inline">\(X\)</span> is roughly a measure of how spread out
it is from its mean. It is given as <span
class="math display">\[\begin{aligned}
    Var(X) = E[(X-E[X])^2] = E[X^2] - (E[X])^2
\end{aligned}\]</span></p>
</section>
<section id="some-examples" class="level2 unnumbered">
<h2 class="unnumbered">Some Examples</h2>
<section id="coin-toss-game" class="level4 unnumbered">
<h4 class="unnumbered">Coin Toss Game</h4>
<p>Assume there are two gamblers playing a coin toss game. Gambler <span
class="math inline">\(A\)</span> has <span
class="math inline">\((n+1)\)</span> fair coins, and <span
class="math inline">\(B\)</span> has <span
class="math inline">\(n\)</span> fair coins. What is the probability
that <span class="math inline">\(A\)</span> will have more heads than
<span class="math inline">\(B\)</span> if both flip all of their
coins?</p>
<p>We cna look at this problem symmetrically if we imagine a slightly
simplified scenario where <span class="math inline">\(A\)</span> and
<span class="math inline">\(B\)</span> both have <span
class="math inline">\(n\)</span> coins. Then, we know that for the
following 3 scenarios</p>
<ul>
<li><p><span class="math inline">\(A\)</span> flips more heads than
<span class="math inline">\(B\)</span>.</p></li>
<li><p><span class="math inline">\(A\)</span> flips the same number of
heads as <span class="math inline">\(B\)</span>.</p></li>
<li><p><span class="math inline">\(A\)</span> flips fewer heads than
<span class="math inline">\(B\)</span>.</p></li>
</ul>
<p>the first and third are symmetric, and so must have equal
probabilities. Then, we really only need to consider the second case,
which is the case where <span class="math inline">\(A\)</span>’s <span
class="math inline">\(n+1\)</span>-th flip would actually make a
difference.</p>
</section>
<section id="card-game" class="level4 unnumbered">
<h4 class="unnumbered">Card Game</h4>
<p>A casino offers a card game from a deck of 52 cards with 4 cards each
for <span class="math inline">\(2,3,4,5,6,7,8,9,10,J,Q,K,A\)</span>. You
pick up a card from the deck and the dealer picks another one with
replacement. What is the probability that you picked a larger card than
the dealer’s?</p>
<p>Well, one way to analyze this is by considering all possible choices
of two cards <span class="math inline">\(C_1\)</span> and <span
class="math inline">\(C_2\)</span>. The probability of pciking any
particular suit for <span class="math inline">\(C_1\)</span> is <span
class="math inline">\(4/52 = 1/13\)</span>. We can choose to then simply
analyze the probability of getting a smaller card for each possible suit
case. Basically, for each suit, the probability of picking a lesser card
for <span class="math inline">\(C_2\)</span> is <span
class="math inline">\(4/51 * k\)</span>, where <span
class="math inline">\(k\)</span> is 1 less than the rank of the suit.
So, in general, we can compute the overall probability as <span
class="math display">\[\begin{aligned}
    \left(\frac{1}{13}*\frac{4}{51}*0\right) +
\left(\frac{1}{13}*\frac{4}{51}*1\right) +
\left(\frac{1}{13}*\frac{4}{51}*2\right) + \dots +
\left(\frac{1}{13}*\frac{4}{51}*12\right) = 0.4705
\end{aligned}\]</span></p>
</section>
</section>
</section>
<section id="conditional-probability-and-bayes-formula" class="level1"
data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span>
Conditional Probability and Bayes’ Formula</h1>
<p>In standard probability notions, we may have some event <span
class="math inline">\(A\)</span> and want to know the probability of
<span class="math inline">\(A\)</span> occurring. If, however, we have
two separate events, <span class="math inline">\(A\)</span> and <span
class="math inline">\(B\)</span>, we can also consider the probability
that one event occurs <em>given</em> that the other occurs. This is
known as conditional probability i.e., for events <span
class="math inline">\(A\)</span> and <span
class="math inline">\(B\)</span>, the conditional probability of <span
class="math inline">\(A\)</span> <em>given</em> <span
class="math inline">\(B\)</span> is defined as <span
class="math display">\[\begin{aligned}
    P(A \mid B) = \dfrac{P(A \cap B)}{P(B)}
\end{aligned}\]</span> You can think of this formula visually:</p>
<div class="center">
<p><img src="diagrams/prob_diagrams/prob_diagrams.001.png"
alt="image" /></p>
</div>
<p>That is, if we consider <span class="math inline">\(P(A |
B)\)</span>, we can imagine this is like saying our sample space is now
shrunk just to <span class="math inline">\(B\)</span>, and so the
probability of <span class="math inline">\(A\)</span> in this reduced
space is dependent on the size of the intersection between <span
class="math inline">\(A\)</span> and <span
class="math inline">\(B\)</span> as a proportion of <span
class="math inline">\(B\)</span>, as represented in the formula.</p>
<section id="bayes-rule" class="level2" data-number="2.1">
<h2 data-number="2.1"><span class="header-section-number">2.1</span>
Bayes’ Rule</h2>
<p><strong>Bayes’ rule</strong> provides an alternate expression for
conditional probability. For two events <span
class="math inline">\(A\)</span> and <span
class="math inline">\(B\)</span>, it states: <span
class="math display">\[\begin{aligned}
    P(A \mid B) = \dfrac{P(B|A) * P(A)}{P(B)}
\end{aligned}\]</span> You can think of this as deriving from the basic
conditional probability statement above since <span
class="math inline">\(P(B \mid A)\)</span> can be viewed as the
probability of the intersection between <span
class="math inline">\(A\)</span> and <span
class="math inline">\(B\)</span> as a proportion to <span
class="math inline">\(A\)</span> as our sample space, and then we scale
this up by factor of <span class="math inline">\(P(A)\)</span> to
explicitly get <span class="math inline">\(P(A \cap B)\)</span> in the
numerator.</p>
</section>
<section id="examples" class="level2 unnumbered">
<h2 class="unnumbered">Examples</h2>
<p>For example, if I roll a fair 6-sided die, then the probability of
rolling a 4 is <span class="math inline">\(1/6\)</span>. But, if I tell
you upfront that the die roll seen is <span class="math inline">\(&gt;=
4\)</span>, then the probability that the roll is a 4 is actually now
<span class="math inline">\(1/3\)</span>, since there are now only 3
possible outcomes in the reduced/conditioned sample sample (<span
class="math inline">\(\{4,5,6\}\)</span>) and our outcome 4 is one of
them.</p>
</section>
</section>
<section id="markov-chains" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Markov
Chains</h1>
<p>A <strong>Markov chain</strong> is a sequence of random variables
<span class="math inline">\(X_0,X_1,\dots,X_n,\dots\)</span> with the
Markov property that, given the present state, the future states and
past states are independent. In other words, once the current state is
known, past history has no bearing on the future. A Markov chain with
<span class="math inline">\(M\)</span> states can be completely
described by an <span class="math inline">\(M \times M\)</span>
transition matrix <span class="math inline">\(P\)</span> and the initial
probabilities <span class="math inline">\(P(X_0)\)</span>, where <span
class="math inline">\(p_{ij}\)</span> is the probability of
transitioning from state <span class="math inline">\(i\)</span> to state
<span class="math inline">\(j\)</span>. The probability of a particular
path of states then, is the product of the probabilities of taking each
corresponding transition in that path.</p>
<p>A <strong>transition graph</strong> is more commonly used to
graphically represent the transition matrix i.e.</p>
<div class="center">
<p><img src="diagrams/markov_chain_graph.png" alt="image" /></p>
</div>
<p>Note that a state <span class="math inline">\(i\)</span> is called
<strong>absorbing</strong> if it is impossible to leave this state.</p>
<section id="example-gamblers-ruin" class="level4 unnumbered">
<h4 class="unnumbered">Example: Gambler’s Ruin</h4>
<p>As an example, consider the gambler’s ruin problem. Say player <span
class="math inline">\(M\)</span> has $1 and play N has $2. Each game
gives the winner of that game $1 from the other. As a better player,
<span class="math inline">\(M\)</span> wins 2/3 of the games, and they
play until one of them is bankrupt. What is the probability that <span
class="math inline">\(M\)</span> wins?</p>
<p>We can model this as a Markov chain where states are tuples of the
account values of <span class="math inline">\(M\)</span> and <span
class="math inline">\(N\)</span>, with initial state of <span
class="math inline">\((1,2)\)</span>. At each state, there are then two
possible transitions: one where <span class="math inline">\(M\)</span>
wins and one where <span class="math inline">\(N\)</span> wins, the
former having probability <span class="math inline">\(2/3\)</span> and
the latter having probability <span class="math inline">\(1/3\)</span>.
The <span class="math inline">\(M\)</span> winning transition goes from
a state <span class="math inline">\((m,n)\)</span> to <span
class="math inline">\((m+1,n-1)\)</span>, and similarly for the <span
class="math inline">\(N\)</span> winning transition. A winning (losing)
state is one where one player’s account balance is <span
class="math inline">\(\leq 0\)</span>. So, we want to determine the
probability of reaching a state where <span
class="math inline">\(M\)</span> wins, starting from the initial
state.</p>
<p>The Markov chain (i.e. probabilistic transition system) for this
problem can be drawn as follows (where bankrupt values are shown in
red):</p>
<div class="center">
<p><img src="diagrams/prob_diagrams/prob_diagrams.002.png"
alt="image" /></p>
</div>
</section>
</section>
<section id="basic-calculus" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Basic
Calculus</h1>
<section id="common-derivative-rules" class="level3 unnumbered">
<h3 class="unnumbered">Common Derivative Rules</h3>
<ul>
<li><p><strong>Power Rule</strong>: For <span class="math inline">\(f(x)
= ax^n\)</span>, we have <span class="math display">\[f&#39;(x) = n
\cdot a x^{n-1}\]</span></p></li>
<li><p><strong>Product Rule</strong>: For functions <span
class="math inline">\(f(x)\)</span> and <span
class="math inline">\(g(x)\)</span>, we have <span
class="math display">\[(f \cdot g)&#39; = f&#39;g + fg&#39;\]</span>
i.e., the derivative of the product of <span
class="math inline">\(f\)</span> and <span
class="math inline">\(g\)</span> can be remembered as “the derivative of
<span class="math inline">\(f\)</span> times <span
class="math inline">\(g\)</span> plus <span
class="math inline">\(f\)</span> times the derivative of <span
class="math inline">\(g\)</span>”.</p></li>
<li><p><strong>Chain Rule</strong>: For two functions <span
class="math inline">\(f(x)\)</span> and <span
class="math inline">\(g(x)\)</span>, and their composition <span
class="math inline">\(h(x) = f(g(x))\)</span>, we have <span
class="math display">\[\begin{aligned}
        h&#39;(x) = f&#39;(g(x))g&#39;(x)
    
\end{aligned}\]</span> Alternately stated as: <span
class="math display">\[\begin{aligned}
        (f \circ g)&#39; = (f&#39; \circ g) \cdot g&#39;
    
\end{aligned}\]</span></p></li>
</ul>
</section>
<section id="derivatives-of-common-functions" class="level3 unnumbered">
<h3 class="unnumbered">Derivatives of Common Functions</h3>
<ul>
<li><p><span class="math inline">\(\frac{d}{dx} (\ln{x}) =
\frac{1}{x}\)</span></p></li>
<li><p><span class="math inline">\(\frac{d}{dx} (c^x) = c^x
\ln{c}\)</span></p></li>
<li><p><span class="math inline">\(\frac{d}{dx} (e^{x}) =
{e}^{x}\)</span></p></li>
<li><p><span class="math inline">\(\frac{d}{dx} (\sin{x}) =
\cos{x}\)</span></p></li>
<li><p><span class="math inline">\(\frac{d}{dx} (\cos{x}) =
-\sin{x}\)</span></p></li>
</ul>
</section>
</section>
</body>
</html>
