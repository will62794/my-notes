<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="William Schultz" />
  <meta name="author" content="William Schultz" />
  <title>Notes on Theory of Computation</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <link rel="stylesheet" href="../../style.css" />
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Notes on Theory of Computation</h1>
<p class="author">William Schultz</p>
<p class="author">William Schultz</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#decidability"><span class="toc-section-number">1</span> Decidability</a></li>
<li><a href="#p-and-np"><span class="toc-section-number">2</span> P and NP</a>
<ul>
<li><a href="#np-completeness"><span class="toc-section-number">2.1</span> NP-Completeness</a></li>
<li><a href="#np-vs.-nexp"><span class="toc-section-number">2.2</span> NP vs. NEXP</a></li>
</ul></li>
<li><a href="#upper-and-lower-bounds"><span class="toc-section-number">3</span> Upper and Lower Bounds</a>
<ul>
<li><a href="#establishing-lower-bounds">Establishing Lower Bounds</a></li>
</ul></li>
<li><a href="#complexity-classes">Complexity Classes</a></li>
<li><a href="#bibliography">References</a></li>
</ul>
</nav>
<p>In standard computation/complexity theory, we study <em>decision problems</em>. That is, computational problems that have a <em>yes/no</em> answer. These types of problems can be formulated in terms of a corresponding <em>language</em>, which is a set of strings <span class="math inline">\(w \in \Sigma^*\)</span>, where <span class="math inline">\(\Sigma\)</span> is some finite alphabet i.e. some finite set of symbols.</p>
<section id="decidability" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Decidability</h1>
<p>We say that a language is <em>decidable</em> if there exists a Turing machine <span class="math inline">\(M\)</span> such that <span class="math inline">\(M\)</span> accepts every input <span class="math inline">\(w \in L\)</span> and always halts. We say that a language is <em>recognizable</em> if there is a Turing machine <span class="math inline">\(M\)</span> that accepts every input <span class="math inline">\(w \in L\)</span> but it may reject or loop forever on inputs <span class="math inline">\(w \notin L\)</span>. Note that for any Turing machine on a given input, there are three possible outcomes: (1) <em>accept</em>, (2) <em>reject</em>, or (3) <em>loop forever</em>. A basic undecidable problem is the <span class="math inline">\(A_{TM}\)</span> problem, which asks, given a Turing machine <span class="math inline">\(M\)</span> and input <span class="math inline">\(w\)</span>, does machine <span class="math inline">\(M\)</span> accept on <span class="math inline">\(w\)</span>? Note that the class of recognizable languages is more general than the class of undecidable languages. That is, any language that is decidable is recognizable, but the converse is not necessarily true. Decidability is in some sense the coarsest possible notion of complexity i.e. it is only concerned with whether there exists any general algorithm for solving a problem, regardless of how fast/slow it is. Undeciable problems are, in some sense, outside the realm of what can be computed by a single, generic algorithm i.e. one that successfully terminates on all possible inputs.</p>
</section>
<section id="p-and-np" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> P and NP</h1>
<p>The class <span class="math inline">\(P\)</span> is the set of languages for which there exists a Turing machine (i.e. an algorithm) that can decide <span class="math inline">\(P\)</span> in polynomial time <span class="math inline">\(O(n^k)\)</span>, for some positive constant <span class="math inline">\(k\)</span>. The class <span class="math inline">\(NP\)</span> is defined as the set of languages for which there exists a polynomial time <em>verifier</em>. Formally, a language <span class="math inline">\(L\)</span> is in <span class="math inline">\(NP\)</span> if there exists a Turing machine <span class="math inline">\(V\)</span> (a verifier) such that, for any input <span class="math inline">\(w \in L\)</span> there exists a certificate <span class="math inline">\(c\)</span> such that <span class="math inline">\(V\)</span> accepts <span class="math inline">\(\langle w,c \rangle\)</span> and runs in polynomial time. Note that the size of the certificate can only be polynomial in the size of the input <span class="math inline">\(w\)</span>, since the verifier machine only has a polynomial time run-time budget. At a high level, <span class="math inline">\(NP\)</span> is the class of languages for which there exists an efficient way to verify solutions to a given problem instance. There may not be an efficient algorithm to decide the answer to a given instance though. It is not known for certain, however, whether there are (or aren’t) polynomial time algorithms for solving problems in <span class="math inline">\(NP\)</span>. This is the famous <span class="math inline">\(P\)</span> vs. <span class="math inline">\(NP\)</span> problem. It is believed that problems in <span class="math inline">\(NP\)</span> do not have efficient (polynomial time) algorithms, but this is not formally proven.</p>
<section id="np-completeness" class="level2" data-number="2.1">
<h2 data-number="2.1"><span class="header-section-number">2.1</span> NP-Completeness</h2>
<p>There are some problems in the class <span class="math inline">\(NP\)</span> that are the “hardest” problems in <span class="math inline">\(NP\)</span>. We call these problems <span class="math inline">\(NP\)</span>-complete. Formally, we say that a language <span class="math inline">\(A\)</span> is <span class="math inline">\(NP\)</span>-complete if <span class="math inline">\(A \in NP\)</span> and every language <span class="math inline">\(B \in NP\)</span> is polynomial time reducible to <span class="math inline">\(A\)</span>. For two languages <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, we say that <span class="math inline">\(A\)</span> is polynomial time reducible to <span class="math inline">\(B\)</span> if there is a polynomial time converter <span class="math inline">\(R_{A\rightarrow B}\)</span> that converts an input <span class="math inline">\(w_A\)</span> to an input <span class="math inline">\(w_B = R_{A\rightarrow B}(w_A)\)</span> so that <span class="math inline">\(w_B \in B \iff w_A \in A\)</span>. So, if a problem is <span class="math inline">\(NP\)</span>-complete, it means that every problem in <span class="math inline">\(NP\)</span> can be reduced to it i.e. we can take an input of any problem in <span class="math inline">\(NP\)</span>, convert it to an input for the NP-complete problem in a way that preserves correctness. So, this means that if we solve one <span class="math inline">\(NP\)</span> complete problem in an efficient (polynomial time) algorithm, then all <span class="math inline">\(NP\)</span> problems are efficiently solvable.</p>
<p>The canonical <span class="math inline">\(NP\)</span>-complete problem is the SAT problem i.e. checking whether a boolean formula is satisfiable. The Cook-Levin theorem shows that SAT is <span class="math inline">\(NP\)</span>-complete. With this knowledge, we can prove other problems <span class="math inline">\(NP\)</span>-complete. Since we know that any <span class="math inline">\(B \in NP\)</span> can be reduced to an <span class="math inline">\(NP\)</span>-complete problem <span class="math inline">\(A\)</span>, we can show that <span class="math inline">\(B\)</span> is <span class="math inline">\(NP\)</span>-complete by showing that <span class="math inline">\(A\)</span> is reducible to <span class="math inline">\(B\)</span>. That is, we establish that <span class="math inline">\(A\)</span> is reducible to <span class="math inline">\(B\)</span> and <span class="math inline">\(B\)</span> is reducible to <span class="math inline">\(A\)</span>, so the problems are “equivalently" hard. Note that there are some problems in <span class="math inline">\(NP\)</span>, however, that are not <span class="math inline">\(NP\)</span>-complete. Ladner’s theorem establishes this i.e. it proves the existence of problems that are in <span class="math inline">\(NP\)</span> but not in <span class="math inline">\(P\)</span> and are not <span class="math inline">\(NP\)</span>-complete. This class is called <span class="math inline">\(NP\)</span>-intermediate. The construction used in this theorem is complicated, though, and not necessarily “natural”. There are problems that are suspected to be in <span class="math inline">\(NP\)</span>-intermediate i.e. they are in <span class="math inline">\(NP\)</span> but have not been shown to be in <span class="math inline">\(P\)</span> or be <span class="math inline">\(NP\)</span>-complete e.g. integer factorization.</p>
</section>
<section id="np-vs.-nexp" class="level2" data-number="2.2">
<h2 data-number="2.2"><span class="header-section-number">2.2</span> NP vs. NEXP</h2>
<p>I believe that one straightforward way to characterize the difference between “polynomial" non-deterministic Turing machines (NP) and “exponential" non-deterministic Turing machines (NEXP) is to consider the maximum depth of the computation tree branches. In NP, we are allowed to use non-deterministic machines with paths of polynomial length, whereas in NEXP, we are allowed to use non-deterministic machines with exponentially long paths.</p>
<p>Note that we can also use the nondeterministic Turing machine perspective as an alternate way to view the difference between classes like <span class="math inline">\(NP\)</span> and <span class="math inline">\(coNP\)</span>. <span class="math inline">\(NP\)</span> consists of problems that have some accepting path out of all branches on the nondeterministic computation, whereas problems in <span class="math inline">\(coNP\)</span> are those for which all paths must accept. For example, satisfiability vs. unsatisfiability.</p>
</section>
</section>
<section id="upper-and-lower-bounds" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Upper and Lower Bounds</h1>
<p>For any decision problem, we can establish both <em>upper bounds</em> and <em>lower bounds</em> on its complexity. Recall that a decision problem is formulated in terms of a language <span class="math inline">\(L\)</span>, consisting of a set of strings. The decision problem for a given language <span class="math inline">\(L\)</span> is to determine whether <span class="math inline">\(w \in L\)</span> for some given string <span class="math inline">\(w\)</span>.</p>
<p>An upper bound makes a statement about the maximum hardness/complexity of the problem, and a lower bound makes a statement about the minimum easiness of the problem. Establishing an upper bound is typically much easier, since you only need to provide a concrete algorithm that solves the problem in some worst case running time (i.e. show there exists an algorithm). Establishing a lower bound is generally much harder, since you need to show that there exists no algorithm that can solve the problem more efficiently than a certain complexity class (i.e. show no algorithm exists).</p>
<section id="establishing-lower-bounds" class="level3 unnumbered">
<h3 class="unnumbered">Establishing Lower Bounds</h3>
<p>For example, for comparison based sorting algorithms, we can show a lower bound of <span class="math inline">\(\Omega(n \log{n})\)</span> for the worst case complexity. That is, no comparison based sorting algorithm can exist with worst case complexity better than <span class="math inline">\(n \log{n}\)</span>.</p>
<p>As another example, one of the best known algorithms for 3-SAT as of 2019 has a numerical upper bound of something around <span class="math inline">\(O(1.307^n)\)</span> <span class="citation" data-cites="2019fasterkSAT">(<a href="#ref-2019fasterkSAT" role="doc-biblioref">Hansen et al. 2019</a>)</span>. There exists no known, general algorithm that can solve 3-SAT in polynomial time. But, it has also not been proven that such an algorithm doesn’t exist. It seems that the best known lower bounds for SAT sit somewhere in the polynomial range of <span class="math inline">\(n^{1.801}\)</span>, though this has some other caveats about “time-space tradeoffs” which I don’t fully understand <span class="citation" data-cites="SE3satlowerbounds">(<a href="#ref-SE3satlowerbounds" role="doc-biblioref"><span>“<span class="nocase">What are the best current lower bounds on 3SAT?</span>”</span> n.d.</a>)</span>. Proving that SAT, for example, had an exponential (or even super polynomial) lower bound would establish that <span class="math inline">\(P \neq NP\)</span>, since SAT is NP-complete, and this would serve to separate <span class="math inline">\(P\)</span> from <span class="math inline">\(NP\)</span>. Of course, one could also prove <span class="math inline">\(P=NP\)</span> by simply giving a polynomial time algorithm for SAT i.e. by dropping the upper bound from exponential to polynomial. This might be “easier”, in the sense that you would only have to find a single algorithm, but it may be “harder" in the sense that P=NP may not actually be true.</p>
</section>
</section>
<section id="complexity-classes" class="level1 unnumbered">
<h1 class="unnumbered">Complexity Classes</h1>
<div class="center">
<p><embed src="diagrams/complexity-classes.pdf" /></p>
</div>
<p>TODO: Polynomial hierarchy.</p>
</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-2019fasterkSAT" class="csl-entry" role="doc-biblioentry">
Hansen, Thomas Dueholm, Haim Kaplan, Or Zamir, and Uri Zwick. 2019. <span>“<span class="nocase">Faster k-SAT Algorithms Using Biased-PPSZ</span>.”</span> In <em>Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing</em>, 578–89. STOC 2019. New York, NY, USA: Association for Computing Machinery. <a href="https://doi.org/10.1145/3313276.3316359">https://doi.org/10.1145/3313276.3316359</a>.
</div>
<div id="ref-SE3satlowerbounds" class="csl-entry" role="doc-biblioentry">
<span>“<span class="nocase">What are the best current lower bounds on 3SAT?</span>”</span> n.d. <a href="https://cstheory.stackexchange.com/questions/93/what-are-the-best-current-lower-bounds-on-3sat">https://cstheory.stackexchange.com/questions/93/what-are-the-best-current-lower-bounds-on-3sat</a>. <a href="https://cstheory.stackexchange.com/questions/93/what-are-the-best-current-lower-bounds-on-3sat">https://cstheory.stackexchange.com/questions/93/what-are-the-best-current-lower-bounds-on-3sat</a>.
</div>
</div>
</section>
</body>
</html>
