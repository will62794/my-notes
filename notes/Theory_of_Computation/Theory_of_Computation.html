<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="William Schultz" />
  <meta name="author" content="William Schultz" />
  <title>Notes on Theory of Computation</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <link rel="stylesheet" href="../../style.css" />
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Notes on Theory of Computation</h1>
<p class="author">William Schultz</p>
<p class="author">William Schultz</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#regular-languages"><span class="toc-section-number">1</span> Regular Languages</a></li>
<li><a href="#decidability"><span class="toc-section-number">2</span> Decidability</a></li>
<li><a href="#p-and-np"><span class="toc-section-number">3</span> P and NP</a>
<ul>
<li><a href="#np-completeness"><span class="toc-section-number">3.1</span> NP-Completeness</a></li>
<li><a href="#np-vs.-nexp"><span class="toc-section-number">3.2</span> NP vs. NEXP</a></li>
</ul></li>
<li><a href="#upper-and-lower-bounds"><span class="toc-section-number">4</span> Upper and Lower Bounds</a>
<ul>
<li><a href="#establishing-lower-bounds">Establishing Lower Bounds</a></li>
<li><a href="#more-on-lower-bounds">More on Lower Bounds</a></li>
</ul></li>
<li><a href="#complexity-class-hierarchy"><span class="toc-section-number">5</span> Complexity Class Hierarchy</a></li>
<li><a href="#probabilistic-tools"><span class="toc-section-number">6</span> Probabilistic Tools</a>
<ul>
<li><a href="#linearity-of-expectation">Linearity of Expectation</a></li>
<li><a href="#expectation-and-variance">Expectation and Variance</a></li>
<li><a href="#markovs-inequality">Markov’s Inequality</a></li>
<li><a href="#chebyshevs-inequality">Chebyshev’s Inequality</a></li>
<li><a href="#chernoff-bound">Chernoff Bound</a></li>
<li><a href="#union-bound">Union Bound</a></li>
</ul></li>
<li><a href="#bibliography">References</a></li>
</ul>
</nav>
<p>In standard computation/complexity theory, we study <em>decision problems</em>. That is, computational problems that have a <em>yes/no</em> answer. These types of problems can be formulated in terms of a corresponding <em>language</em>, which is a set of strings <span class="math inline">\(w \in \Sigma^*\)</span>, where <span class="math inline">\(\Sigma\)</span> is some finite alphabet i.e. some finite set of symbols.</p>
<section id="regular-languages" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Regular Languages</h1>
<p>Formally, a <em>language</em> is a set of strings <span class="math inline">\(w \in \Sigma^*\)</span>, where <span class="math inline">\(\Sigma\)</span> is some finite alphabet i.e. some finite set of atomic symbols. A language <span class="math inline">\(L\)</span> is <em>regular</em> if it can be accepted by some deterministic finite automaton (DFA). That is, there is some DFA that can determine, for any string <span class="math inline">\(w \in \Sigma^*\)</span>, whether <span class="math inline">\(w \in L\)</span> or <span class="math inline">\(w \notin L\)</span>. A <em>regular expression</em> is one way to represent a regular language. It provides a concise form for expressing regular languages i.e. it defines a language as the set of strings accepted by that regular expression.</p>
<p>Any regular expression can be converted into an equivalent nondeterministic finite automaton (NFA), or an equivalent DFA. Recall that the translation from an NFA to DFA may, in the worst case, lead to an exponential blow up in the number of states. See Thompson’s Construction algorithm for transforming a regular expression into an equivalent NFA, which can then be converted to a DFA e.g. via the <em>powerset construction</em>.</p>
</section>
<section id="decidability" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Decidability</h1>
<p>We say that a language is <em>decidable</em> if there exists a Turing machine <span class="math inline">\(M\)</span> such that <span class="math inline">\(M\)</span> accepts every input <span class="math inline">\(w \in L\)</span> and always halts. We say that a language is <em>recognizable</em> if there is a Turing machine <span class="math inline">\(M\)</span> that accepts every input <span class="math inline">\(w \in L\)</span> but it may reject or loop forever on inputs <span class="math inline">\(w \notin L\)</span>. Note that for any Turing machine on a given input, there are three possible outcomes: (1) <em>accept</em>, (2) <em>reject</em>, or (3) <em>loop forever</em>. A basic undecidable problem is the <span class="math inline">\(A_{TM}\)</span> problem, which asks, given a Turing machine <span class="math inline">\(M\)</span> and input <span class="math inline">\(w\)</span>, does machine <span class="math inline">\(M\)</span> accept on <span class="math inline">\(w\)</span>? Note that the class of recognizable languages is more general than the class of undecidable languages. That is, any language that is decidable is recognizable, but the converse is not necessarily true. Decidability is in some sense the coarsest possible notion of complexity i.e. it is only concerned with whether there exists any general algorithm for solving a problem, regardless of how fast/slow it is. Undeciable problems are, in some sense, outside the realm of what can be computed by a single, generic algorithm i.e. one that successfully terminates on all possible inputs.</p>
</section>
<section id="p-and-np" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> P and NP</h1>
<p>The class <span class="math inline">\(P\)</span> is the set of languages for which there exists a Turing machine (i.e. an algorithm) that can decide <span class="math inline">\(P\)</span> in polynomial time <span class="math inline">\(O(n^k)\)</span>, for some positive constant <span class="math inline">\(k\)</span>. The class <span class="math inline">\(NP\)</span> is defined as the set of languages for which there exists a polynomial time <em>verifier</em>. Formally, a language <span class="math inline">\(L\)</span> is in <span class="math inline">\(NP\)</span> if there exists a Turing machine <span class="math inline">\(V\)</span> (a verifier) such that, for any input <span class="math inline">\(w \in L\)</span> there exists a certificate <span class="math inline">\(c\)</span> such that <span class="math inline">\(V\)</span> accepts <span class="math inline">\(\langle w,c \rangle\)</span> and runs in polynomial time. Note that the size of the certificate can only be polynomial in the size of the input <span class="math inline">\(w\)</span>, since the verifier machine only has a polynomial time run-time budget. At a high level, <span class="math inline">\(NP\)</span> is the class of languages for which there exists an efficient way to verify solutions to a given problem instance. There may not be an efficient algorithm to decide the answer to a given instance though. It is not known for certain, however, whether there are (or aren’t) polynomial time algorithms for solving problems in <span class="math inline">\(NP\)</span>. This is the famous <span class="math inline">\(P\)</span> vs. <span class="math inline">\(NP\)</span> problem. It is believed that problems in <span class="math inline">\(NP\)</span> do not have efficient (polynomial time) algorithms, but this is not formally proven.</p>
<section id="np-completeness" class="level2" data-number="3.1">
<h2 data-number="3.1"><span class="header-section-number">3.1</span> NP-Completeness</h2>
<p>There are some problems in the class <span class="math inline">\(NP\)</span> that are the “hardest” problems in <span class="math inline">\(NP\)</span>. We call these problems <span class="math inline">\(NP\)</span>-complete. Formally, we say that a language <span class="math inline">\(A\)</span> is <span class="math inline">\(NP\)</span>-complete if <span class="math inline">\(A \in NP\)</span> and every language <span class="math inline">\(B \in NP\)</span> is polynomial time reducible to <span class="math inline">\(A\)</span>. For two languages <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, we say that <span class="math inline">\(A\)</span> is polynomial time reducible to <span class="math inline">\(B\)</span> if there is a polynomial time converter <span class="math inline">\(R_{A\rightarrow B}\)</span> that converts an input <span class="math inline">\(w_A\)</span> to an input <span class="math inline">\(w_B = R_{A\rightarrow B}(w_A)\)</span> so that <span class="math inline">\(w_B \in B \iff w_A \in A\)</span>. So, if a problem is <span class="math inline">\(NP\)</span>-complete, it means that every problem in <span class="math inline">\(NP\)</span> can be reduced to it i.e. we can take an input of any problem in <span class="math inline">\(NP\)</span>, convert it to an input for the NP-complete problem in a way that preserves correctness. So, this means that if we solve one <span class="math inline">\(NP\)</span> complete problem in an efficient (polynomial time) algorithm, then all <span class="math inline">\(NP\)</span> problems are efficiently solvable.</p>
<p>The canonical <span class="math inline">\(NP\)</span>-complete problem is the SAT problem i.e. checking whether a boolean formula is satisfiable. The Cook-Levin theorem shows that SAT is <span class="math inline">\(NP\)</span>-complete. With this knowledge, we can prove other problems <span class="math inline">\(NP\)</span>-complete. Since we know that any <span class="math inline">\(B \in NP\)</span> can be reduced to an <span class="math inline">\(NP\)</span>-complete problem <span class="math inline">\(A\)</span>, we can show that <span class="math inline">\(B\)</span> is <span class="math inline">\(NP\)</span>-complete by showing that <span class="math inline">\(A\)</span> is reducible to <span class="math inline">\(B\)</span>. That is, we establish that <span class="math inline">\(A\)</span> is reducible to <span class="math inline">\(B\)</span> and <span class="math inline">\(B\)</span> is reducible to <span class="math inline">\(A\)</span>, so the problems are “equivalently" hard. Note that there are some problems in <span class="math inline">\(NP\)</span>, however, that are not <span class="math inline">\(NP\)</span>-complete. Ladner’s theorem establishes this i.e. it proves the existence of problems that are in <span class="math inline">\(NP\)</span> but not in <span class="math inline">\(P\)</span> and are not <span class="math inline">\(NP\)</span>-complete. This class is called <span class="math inline">\(NP\)</span>-intermediate. The construction used in this theorem is complicated, though, and not necessarily “natural”. There are problems that are suspected to be in <span class="math inline">\(NP\)</span>-intermediate i.e. they are in <span class="math inline">\(NP\)</span> but have not been shown to be in <span class="math inline">\(P\)</span> or be <span class="math inline">\(NP\)</span>-complete e.g. integer factorization.</p>
</section>
<section id="np-vs.-nexp" class="level2" data-number="3.2">
<h2 data-number="3.2"><span class="header-section-number">3.2</span> NP vs. NEXP</h2>
<p>I believe that one straightforward way to characterize the difference between “polynomial" non-deterministic Turing machines (NP) and “exponential" non-deterministic Turing machines (NEXP) is to consider the maximum depth of the computation tree branches. In NP, we are allowed to use non-deterministic machines with paths of polynomial length, whereas in NEXP, we are allowed to use non-deterministic machines with exponentially long paths.</p>
<p>Note that we can also use the nondeterministic Turing machine perspective as an alternate way to view the difference between classes like <span class="math inline">\(NP\)</span> and <span class="math inline">\(coNP\)</span>. <span class="math inline">\(NP\)</span> consists of problems that have some accepting path out of all branches on the nondeterministic computation, whereas problems in <span class="math inline">\(coNP\)</span> are those for which all paths must accept. For example, satisfiability vs. unsatisfiability.</p>
</section>
</section>
<section id="upper-and-lower-bounds" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Upper and Lower Bounds</h1>
<p>For any decision problem, we can establish both <em>upper bounds</em> and <em>lower bounds</em> on its complexity. Recall that a decision problem is formulated in terms of a language <span class="math inline">\(L\)</span>, consisting of a set of strings. The decision problem for a given language <span class="math inline">\(L\)</span> is to determine whether <span class="math inline">\(w \in L\)</span> for some given string <span class="math inline">\(w\)</span>.</p>
<p>An upper bound makes a statement about the maximum hardness/complexity of the problem, and a lower bound makes a statement about the minimum easiness of the problem. Establishing an upper bound is typically easier, since you only need to provide a concrete algorithm that solves the problem in some worst case running time (i.e. show there exists an algorithm). Establishing a lower bound is generally much harder, since you need to show that there exists no algorithm that can solve the problem more efficiently than a certain complexity class (i.e. show no algorithm exists).</p>
<section id="establishing-lower-bounds" class="level3 unnumbered">
<h3 class="unnumbered">Establishing Lower Bounds</h3>
<p>At a very high level, to establish a (worst-case) lower bound on a given problem, we can generally view this as establishing a statement of the form <span class="math display">\[\begin{aligned}
    \forall A \, \exists x : A(x) \text{ takes a while to compute}\end{aligned}\]</span> where <span class="math inline">\(A\)</span> is any possible algorithm. More concretely, we might want to, for example, show that <span class="math inline">\(A\)</span> takes at least <span class="math inline">\(\Omega(n^2)\)</span> steps on input <span class="math inline">\(x\)</span>. Typically, if we want to reason about any possible algorithm, we need to fix some precise model of computation. For example, we might only consider the <em>query complexity</em> of an algorithm e.g. measure its complexity in terms of number of bits of the input that it queries during execution. One technique for proving lower bounds is to use an <em>adversary</em>-based argument as an approach to construct the problematic input <span class="math inline">\(x\)</span> to any given algorithm <span class="math inline">\(A\)</span>. We can’t select this input up front, so we need to construct it adaptively i.e. in response to the queries made by the algorithm. By showing the existence of an appropriate adversary that causes the algorithm to run for at least a certain number of steps, we can establish a lower bound. This isn’t the only way to establish lower bounds e.g. we can also use reduction argument, similar to how we prove decidability, NP-completeness, etc.</p>
</section>
<section id="more-on-lower-bounds" class="level3 unnumbered">
<h3 class="unnumbered">More on Lower Bounds</h3>
<p>For comparison based sorting algorithms, we can show a lower bound of <span class="math inline">\(\Omega(n \log{n})\)</span> for the worst case complexity. That is, no comparison based sorting algorithm can exist with worst case complexity better than <span class="math inline">\(n \log{n}\)</span>. As another example, one of the best known algorithms for 3-SAT as of 2019 has a numerical upper bound of something around <span class="math inline">\(O(1.307^n)\)</span> <span class="citation" data-cites="2019fasterkSAT">(<a href="#ref-2019fasterkSAT" role="doc-biblioref">Hansen et al. 2019</a>)</span>. There exists no known, general algorithm that can solve 3-SAT in polynomial time. But, it has also not been proven that such an algorithm doesn’t exist. It seems that the best known lower bounds for SAT sit somewhere in the polynomial range of <span class="math inline">\(n^{1.801}\)</span>, though this has some other caveats about “time-space tradeoffs” which I don’t fully understand <span class="citation" data-cites="SE3satlowerbounds">(<a href="#ref-SE3satlowerbounds" role="doc-biblioref"><span>“<span class="nocase">What are the best current lower bounds on 3SAT?</span>”</span> n.d.</a>)</span>. Proving that SAT, for example, had an exponential (or even super polynomial) lower bound would establish that <span class="math inline">\(P \neq NP\)</span>, since SAT is NP-complete, and this would serve to separate <span class="math inline">\(P\)</span> from <span class="math inline">\(NP\)</span>. Of course, one could also prove <span class="math inline">\(P=NP\)</span> by simply giving a polynomial time algorithm for SAT i.e. by dropping the upper bound from exponential to polynomial. This might be “easier”, in the sense that you would only have to find a single algorithm, but it may be “harder" in the sense that P=NP may not actually be true.</p>
</section>
</section>
<section id="complexity-class-hierarchy" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Complexity Class Hierarchy</h1>
<div class="center">
<p><img src="diagrams/complexity-classes.png" alt="image" /></p>
</div>
<p>TODO: Polynomial hierarchy.</p>
</section>
<section id="probabilistic-tools" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Probabilistic Tools</h1>
<p>A <em>random variable</em> is a variable whose possible values are numerical outcomes of a random phenomenon. Each possible outcome is assigned some probability, with the condition that the sum of probabilities overall possible outcomes must sum to 1.<br />
See <span class="citation" data-cites="Doerr_2019">(<a href="#ref-Doerr_2019" role="doc-biblioref">Doerr 2019</a>)</span> for a reference on some probabilistic tools.</p>
<section id="linearity-of-expectation" class="level2 unnumbered">
<h2 class="unnumbered">Linearity of Expectation</h2>
<p>For any two random variables <span class="math inline">\(X,Y\)</span>, it holds that <span class="math display">\[\begin{aligned}
    \mathbb{E}[X + Y] =  \mathbb{E}[X] + \mathbb{E}[Y]\end{aligned}\]</span></p>
</section>
<section id="expectation-and-variance" class="level2 unnumbered">
<h2 class="unnumbered">Expectation and Variance</h2>
<p>For a discrete random variable <span class="math inline">\(X\)</span> taking values in some set <span class="math inline">\(\Omega \subseteq \mathbb{R}\)</span>, its <em>expectation</em> is defined as <span class="math display">\[\begin{aligned}
    \mathbb{E}\left[X\right] = \sum_{\omega \in \Omega} \omega \cdot Pr\left[X=\omega\right]\end{aligned}\]</span> and its <em>variance</em> is defined as <span class="math display">\[\begin{aligned}
    \text{Var}(X) = \mathbb{E}\left[(X - \mathbb{E}\left[X\right])^2\right]\end{aligned}\]</span> That is, expectation is essentially a weighted sum of the values that the random variable can take on, where each value is weighted by the probability of that event occurring, and variance is essentially a measure of the average deviation of the variable from its expectation/mean.</p>
</section>
<section id="markovs-inequality" class="level2 unnumbered">
<h2 class="unnumbered">Markov’s Inequality</h2>
<p><em>Markov’s inequality</em> is an elementary large deviation bound valid for <em>all</em> non-negative random variables. Let <span class="math inline">\(X\)</span> be a non-negative random variable with <span class="math inline">\(\mathbb{E}\left[X\right] &gt; 0\)</span>. Then, for all <span class="math inline">\(\lambda &gt; 0\)</span> <span class="math display">\[\begin{aligned}
    Pr(X \geq \lambda \mathbb{E}\left[X\right]) \leq \dfrac{1}{\lambda}\end{aligned}\]</span> That is, this establishes a bound, for some given parameter <span class="math inline">\(\lambda\)</span>, on how likely a random variable is to be far away from its mean. For example, if <span class="math inline">\(\lambda = 10\)</span>, then this means that the probability that <span class="math inline">\(X\)</span> is greater than 10 times its mean is <span class="math inline">\(\leq \frac{1}{10}\)</span>. Note that this bound doesn’t take into account anything about the actual distribution (only about its mean), so it may serve as only a rough estimate.</p>
</section>
<section id="chebyshevs-inequality" class="level2 unnumbered">
<h2 class="unnumbered">Chebyshev’s Inequality</h2>
<p>Let <span class="math inline">\(X\)</span> be a random variable with <span class="math inline">\(\text{Var}(X) &gt; 0\)</span>, and where <span class="math inline">\(\sigma = \sqrt{\text{Var}(X)}\)</span> is its standard deviation. Then, for all <span class="math inline">\(\lambda &gt; 0\)</span> <span class="math display">\[\begin{aligned}
    Pr \left[ \left| X - \mathbb{E}\left[X\right] \right| \geq \lambda \sigma \right] \leq \dfrac{1}{\lambda^2}\end{aligned}\]</span> This bound tells us something about how far a random variable is from its expectation, in terms of the variance of the RV. That is, it puts a bound on the probability of how many (<span class="math inline">\(\lambda\)</span>) standard deviations (<span class="math inline">\(\sigma\)</span>) away from its mean <span class="math inline">\(X\)</span> may be.</p>
</section>
<section id="chernoff-bound" class="level2 unnumbered">
<h2 class="unnumbered">Chernoff Bound</h2>
<p>Let <span class="math inline">\(X_1,\dots,X_n\)</span> be independent random variables in <span class="math inline">\([0,1]\)</span>. Let <span class="math inline">\(X = \sum_{i=1}^n X_i\)</span> and let <span class="math inline">\(\mu = \mathbb{E}[X]\)</span>. Then</p>
<ul>
<li><p><strong>Upper Tail</strong> <span class="math inline">\(Pr[X \geq (1+\delta)\mu] \leq \exp\left({-\tfrac{\delta^2}{2+\delta}\mu}\right), \quad  \forall \delta &gt; 0\)</span></p></li>
<li><p><strong>Lower Tail</strong>: <span class="math inline">\(Pr[X \geq (1-\delta)\mu] \leq \exp\left({-\tfrac{\delta^2}{2}\mu}\right), \quad  \forall \delta \in [0,1]\)</span></p></li>
</ul>
<p>The two tail bounds above can be combined into the following (slightly weaker) two-sided tail bound:</p>
<ul>
<li><p><strong>Two-sided bound</strong>: <span class="math inline">\(Pr[|X-\mu| \geq \delta \mu] \leq 2 \exp\left({-\tfrac{\delta^2 \mu}{3}}\right), \quad  \forall \delta \in [0,1]\)</span></p></li>
</ul>
<p>We can also derive the <em>additive</em> form of the bound by setting <span class="math inline">\(t=\delta \mu\)</span>:</p>
<ul>
<li><p><strong>Additive Chernoff bound</strong>: <span class="math inline">\(Pr[|X-\mu| \geq t] \leq 2 \exp \left( {-\tfrac{t^2}{3 \mu}} \right), \quad  \forall t \in [0,\mu]\)</span></p></li>
</ul>
</section>
<section id="union-bound" class="level2 unnumbered">
<h2 class="unnumbered">Union Bound</h2>
<p>Let <span class="math inline">\(E_1,\dots,E_n\)</span> be arbitrary events in some probability space. Then, <span class="math display">\[\begin{aligned}
    Pr \left[ \bigcup_{i=1}^n E_i \right] \leq \sum_{i=1}^n Pr[E_i]\end{aligned}\]</span> That is, for a set of events <span class="math inline">\(E_1,\dots,E_n\)</span>, the probability of at least one event occurring is less than or equal to the sum of the probabilities of the individual events. We can think of the union bound with a simple set-based analogy i.e. if events are viewe as subsets of a potential sample space, then the union of these sets can be no larger than the sum of the individual sets (i.e. since the sets may have some non-empty intersections).</p>
</section>
</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Doerr_2019" class="csl-entry" role="doc-biblioentry">
Doerr, Benjamin. 2019. <span>“Probabilistic Tools for the Analysis of Randomized Optimization Heuristics.”</span> In <em>Natural Computing Series</em>, 1–87. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-030-29414-4_1">https://doi.org/10.1007/978-3-030-29414-4_1</a>.
</div>
<div id="ref-2019fasterkSAT" class="csl-entry" role="doc-biblioentry">
Hansen, Thomas Dueholm, Haim Kaplan, Or Zamir, and Uri Zwick. 2019. <span>“<span class="nocase">Faster k-SAT Algorithms Using Biased-PPSZ</span>.”</span> In <em>Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing</em>, 578–89. STOC 2019. New York, NY, USA: Association for Computing Machinery. <a href="https://doi.org/10.1145/3313276.3316359">https://doi.org/10.1145/3313276.3316359</a>.
</div>
<div id="ref-SE3satlowerbounds" class="csl-entry" role="doc-biblioentry">
<span>“<span class="nocase">What are the best current lower bounds on 3SAT?</span>”</span> n.d. <a href="https://cstheory.stackexchange.com/questions/93/what-are-the-best-current-lower-bounds-on-3sat">https://cstheory.stackexchange.com/questions/93/what-are-the-best-current-lower-bounds-on-3sat</a>. <a href="https://cstheory.stackexchange.com/questions/93/what-are-the-best-current-lower-bounds-on-3sat">https://cstheory.stackexchange.com/questions/93/what-are-the-best-current-lower-bounds-on-3sat</a>.
</div>
</div>
</section>
</body>
</html>
